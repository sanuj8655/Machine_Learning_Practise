{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62146e75",
   "metadata": {},
   "source": [
    "### load the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "36279cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5f25b2",
   "metadata": {},
   "source": [
    "#### data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "05c2e107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load emails from a directory\n",
    "def load_file_from_directort(directory):\n",
    "    import os\n",
    "    data = []\n",
    "    for file_name in os.listdir(directory):\n",
    "        with open(directory + \"/\" + file_name, 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "            # collect the email data into the data\n",
    "            data.append(file.read())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9b22c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all spam emails\n",
    "spam_emails = load_file_from_directort('/tmp/enron1/spam')\n",
    "\n",
    "# get all ham emails\n",
    "ham_emails = load_file_from_directort('/tmp/enron1/ham')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f00e3d",
   "metadata": {},
   "source": [
    "#### collect the spam and ham words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "58864268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_email(email):\n",
    "    # collect all the words here\n",
    "    all_words = []\n",
    "    \n",
    "    # remove the punctuation marks and special symbols\n",
    "    symbols = ['~', '!', '@', '#', '$', '%', '^', '&', '*', '(', ')', ':', ';',\n",
    "                         '-', '_', '+', \"'\", '\"', ',', '.', '[', ']', '<', '>']\n",
    "    \n",
    "    tmp_email = email\n",
    "    \n",
    "    # I love !India.~\n",
    "    # I love !India.\n",
    "    # I love India.\n",
    "    # I love India\n",
    "    for symbol in symbols:\n",
    "        # replace all the symbols with ''\n",
    "        tmp_email = tmp_email.replace(symbol, '')\n",
    "        \n",
    "    \n",
    "    # split the emails into collection of words\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    words = word_tokenize(tmp_email)\n",
    "    \n",
    "    # remove the numbers\n",
    "    numbers = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0']\n",
    "    \n",
    "    for word in words:\n",
    "        tmp_word = word\n",
    "        for number in numbers:\n",
    "            # replace the numbers with ''\n",
    "            tmp_word = tmp_word.replace(number, '')\n",
    "        \n",
    "        # 50 => ''\n",
    "        # the minimum length of every word must be > 2\n",
    "        if len(tmp_word) > 2:\n",
    "            all_words.append(word)\n",
    "    \n",
    "    # convert every word to lowercase\n",
    "    all_words = [w.lower() for w in all_words]\n",
    "    \n",
    "    # lemmatize the word\n",
    "    all_words = [lemmatizer.lemmatize(w) for w in all_words]\n",
    "    \n",
    "    # remove the stop words\n",
    "    all_words = [w for w in all_words if w not in stop_words]\n",
    "    \n",
    "    # return the words an its count\n",
    "    import collections\n",
    "    return collections.Counter(all_words).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c8346786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all the spam word\n",
    "spam_words = [process_email(email) for email in spam_emails]\n",
    "\n",
    "# collect all the ham word\n",
    "ham_words = [process_email(email) for email in ham_emails]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bae15ab",
   "metadata": {},
   "source": [
    "#### collect the words with the class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7aee52ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table \n",
    "# word -> class/label\n",
    "def create_row(word, label):\n",
    "    dictionary = dict(word)\n",
    "    \n",
    "    # add the label\n",
    "    dictionary['email_type'] = label\n",
    "    \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c0c6995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: spam\n",
    "spam_words = [create_row(word, 1) for word in spam_words]\n",
    "\n",
    "# 0: ham\n",
    "ham_words = [create_row(word, 0) for word in ham_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "27b5c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the spam and ham words into one table\n",
    "all_words = spam_words + ham_words\n",
    "\n",
    "import random\n",
    "\n",
    "# shuffle the data\n",
    "random.shuffle(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7cc28b",
   "metadata": {},
   "source": [
    "#### classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2dc02b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0b89bdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gisb  contract  gas  please  enron  final  doc  intrastate  interstate  \\\n",
      "0  11.0      10.0  6.0     6.0    6.0    6.0  6.0         5.0         5.0   \n",
      "1   0.0       0.0  2.0     0.0    0.0    0.0  0.0         0.0         0.0   \n",
      "2   0.0       1.0  1.0     4.0    0.0    0.0  0.0         0.0         0.0   \n",
      "3   0.0       0.0  0.0     1.0    0.0    0.0  0.0         0.0         0.0   \n",
      "4   0.0       0.0  0.0     0.0    0.0    0.0  0.0         0.0         0.0   \n",
      "\n",
      "   anthony  ...  germicide  rickettsia  afro  bloodhound  infantryman  \\\n",
      "0      4.0  ...        0.0         0.0   0.0         0.0          0.0   \n",
      "1      0.0  ...        0.0         0.0   0.0         0.0          0.0   \n",
      "2      0.0  ...        0.0         0.0   0.0         0.0          0.0   \n",
      "3      0.0  ...        0.0         0.0   0.0         0.0          0.0   \n",
      "4      0.0  ...        0.0         0.0   0.0         0.0          0.0   \n",
      "\n",
      "   detention  tinkle  portico  raceway  restorative  \n",
      "0        0.0     0.0      0.0      0.0          0.0  \n",
      "1        0.0     0.0      0.0      0.0          0.0  \n",
      "2        0.0     0.0      0.0      0.0          0.0  \n",
      "3        0.0     0.0      0.0      0.0          0.0  \n",
      "4        0.0     0.0      0.0      0.0          0.0  \n",
      "\n",
      "[5 rows x 42696 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c913c6",
   "metadata": {},
   "source": [
    "### data cleansing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "134dbf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the missing values\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6cdbf9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide x and y\n",
    "x = df.drop('email_type', axis=1)\n",
    "y = df['email_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0d93c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state=123456)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c70d7b",
   "metadata": {},
   "source": [
    "### model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2f3bc3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(max_iter=1000)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "model = LogisticRegressionCV(max_iter=1000)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d85ae312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict all the values\n",
    "y_prediction = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49c7a61",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "23b64312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[732  14]\n",
      " [  6 283]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrixmatrix(y_test, y_prediction)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5719a505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       746\n",
      "           1       0.95      0.98      0.97       289\n",
      "\n",
      "    accuracy                           0.98      1035\n",
      "   macro avg       0.97      0.98      0.98      1035\n",
      "weighted avg       0.98      0.98      0.98      1035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6817a5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
